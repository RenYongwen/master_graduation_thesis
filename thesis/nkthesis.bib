@inproceedings{desplanques_ecapa-tdnn_2020,
	title = {{ECAPA}-{TDNN}: Emphasized Channel Attention, Propagation and Aggregation in {TDNN} Based Speaker Verification},
	url = {http://arxiv.org/abs/2005.07143},
	doi = {10.21437/Interspeech.2020-2650},
	shorttitle = {{ECAPA}-{TDNN}},
	abstract = {Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network ({TDNN}) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to {SE}-{ResNet}, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The {SE} block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed {ECAPA}-{TDNN} architecture significantly outperforms state-of-the-art {TDNN} based systems on the {VoxCeleb} test sets and the 2019 {VoxCeleb} Speaker Recognition Challenge.},
	pages = {3830--3834},
	booktitle = {Interspeech 2020},
	author = {Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
	urldate = {2023-04-12},
	date = {2020-10-25},
	eprinttype = {arxiv},
	eprint = {2005.07143 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:files/3/Desplanques 等 - 2020 - ECAPA-TDNN Emphasized Channel Attention, Propagat.pdf:application/pdf;arXiv.org Snapshot:files/4/2005.html:text/html},
}
@inproceedings{chen_multilingual_2017,
	location = {Okinawa},
	title = {Multilingual bottle-neck feature learning from untranscribed speech},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8269009/},
	doi = {10.1109/ASRU.2017.8269009},
	abstract = {We propose to learn a low-dimensional feature representation for multiple languages without access to their manual transcription. The multilingual features are extracted from a shared bottleneck layer of a multi-task learning deep neural network which is trained using unsupervised phoneme-like labels. The unsupervised phoneme-like labels are obtained from language-dependent Dirichlet process Gaussian mixture models ({DPGMMs}). Vocal tract length normalization ({VTLN}) is applied to mel-frequency cepstral coefﬁcients to reduce talker variation when {DPGMMs} are trained. The proposed features are evaluated using the {ABX} phoneme discriminability test in the Zero Resource Speech Challenge 2017. In the experiments, we show that the proposed features perform well across different languages, and they consistently outperform our previously proposed {DPGMM} posteriorgrams which topped the performance in the same challenge in 2015.},
	eventtitle = {2017 {IEEE} Automatic Speech Recognition and Understanding Workshop ({ASRU})},
	pages = {727--733},
	booktitle = {2017 {IEEE} Automatic Speech Recognition and Understanding Workshop ({ASRU})},
	publisher = {{IEEE}},
	author = {Chen, Hongjie and Leung, Cheung-Chi and Xie, Lei and Ma, Bin and Li, Haizhou},
	urldate = {2023-04-27},
	date = {2017-12},
	langid = {english},
	file = {Chen 等 - 2017 - Multilingual bottle-neck feature learning from unt.pdf:files/29/Chen 等 - 2017 - Multilingual bottle-neck feature learning from unt.pdf:application/pdf},
}

@article{li_spoken_2013,
	title = {Spoken Language Recognition: From Fundamentals to Practice},
	volume = {101},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/6451097/},
	doi = {10.1109/JPROC.2012.2237151},
	shorttitle = {Spoken Language Recognition},
	abstract = {Spoken language recognition refers to the automatic process through which we determine or verify the identity of the language spoken in a speech sample. We study a computational framework that allows such a decision to be made in a quantitative manner. In recent decades, we have made tremendous progress in spoken language recognition, which benefited from technological breakthroughs in related areas, such as signal processing, pattern recognition, cognitive science, and machine learning. In this paper, we attempt to provide an introductory tutorial on the fundamentals of the theory and the state-of-the-art solutions, from both phonological and computational aspects. We also give a comprehensive review of current trends and future research directions using the language recognition evaluation ({LRE}) formulated by the National Institute of Standards and Technology ({NIST}) as the case studies.},
	pages = {1136--1159},
	number = {5},
	journaltitle = {Proc. {IEEE}},
	author = {Li, Haizhou and Ma, Bin and Lee, Kong Aik},
	urldate = {2023-04-27},
	date = {2013-05},
	langid = {english},
	file = {Li 等 - 2013 - Spoken Language Recognition From Fundamentals to .pdf:files/31/Li 等 - 2013 - Spoken Language Recognition From Fundamentals to .pdf:application/pdf},
}

@inproceedings{yu_language_2021,
	title = {Language Recognition Based on Unsupervised Pretrained Models},
	url = {https://www.isca-speech.org/archive/interspeech_2021/yu21b_interspeech.html},
	doi = {10.21437/Interspeech.2021-807},
	abstract = {Unsupervised pretrained models have been proven to rival or even outperform supervised systems in various speech recognition tasks. However, their performance for language recognition is still left to be explored. In this paper, we construct several language recognition systems based on existing unsupervised pretraining approaches, and explore their credibility and performance to learn high-level generalization of language. We discover that unsupervised pretrained models capture expressive and highly linear-separable features. With these representations, language recognition can perform well even when the classiﬁers are relatively simple or only a small amount of labeled data is available. Although linear classiﬁers are usable, neural nets with {RNN} structures improve the results. Meanwhile, unsupervised pretrained models are able to gain reﬁned representations on audio frame level that are strongly coupled with the acoustic features of the input sequence. Therefore these features contain redundant information of speakers and channels with few relations to the identity of the language. This nature of unsupervised pretrained models causes a performance degradation in language recognition tasks on crosschannel tests.},
	eventtitle = {Interspeech 2021},
	pages = {3271--3275},
	booktitle = {Interspeech 2021},
	publisher = {{ISCA}},
	author = {Yu, Haibin and Zhao, Jing and Yang, Song and Wu, Zhongqin and Nie, Yuting and Zhang, Wei-Qiang},
	urldate = {2023-04-27},
	date = {2021-08-30},
	langid = {english},
	file = {Yu 等 - 2021 - Language Recognition Based on Unsupervised Pretrai.pdf:files/32/Yu 等 - 2021 - Language Recognition Based on Unsupervised Pretrai.pdf:application/pdf},
}

@misc{bai_speaker_2021,
	title = {Speaker Recognition Based on Deep Learning: An Overview},
	url = {http://arxiv.org/abs/2012.00931},
	shorttitle = {Speaker Recognition Based on Deep Learning},
	abstract = {Speaker recognition is a task of identifying persons from their voices. Recently, deep learning has dramatically revolutionized speaker recognition. However, there is lack of comprehensive reviews on the exciting progress. In this paper, we review several major subtasks of speaker recognition, including speaker veriﬁcation, identiﬁcation, diarization, and robust speaker recognition, with a focus on deep-learning-based methods. Because the major advantage of deep learning over conventional methods is its representation ability, which is able to produce highly abstract embedding features from utterances, we ﬁrst pay close attention to deep-learning-based speaker feature extraction, including the inputs, network structures, temporal pooling strategies, and objective functions respectively, which are the fundamental components of many speaker recognition subtasks. Then, we make an overview of speaker diarization, with an emphasis of recent supervised, end-to-end, and online diarization. Finally, we survey robust speaker recognition from the perspectives of domain adaptation and speech enhancement, which are two major approaches of dealing with domain mismatch and noise problems. Popular and recently released corpora are listed at the end of the paper.},
	number = {{arXiv}:2012.00931},
	publisher = {{arXiv}},
	author = {Bai, Zhongxin and Zhang, Xiao-Lei},
	urldate = {2023-04-27},
	date = {2021-04-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2012.00931 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Bai 和 Zhang - 2021 - Speaker Recognition Based on Deep Learning An Ove.pdf:files/33/Bai 和 Zhang - 2021 - Speaker Recognition Based on Deep Learning An Ove.pdf:application/pdf},
}

@article{kang_deep_nodate,
	title = {Deep learning-based end-to-end spoken language identification system for domain-mismatched scenario},
	abstract = {Domain mismatch is a critical issue when it comes to spoken language identification. To overcome the domain mismatch problem, we have applied several architectures and deep learning strategies which have shown good results in cross-domain speaker verification tasks to spoken language identification. Our systems were evaluated on the Oriental Language Recognition ({OLR}) Challenge 2021 Task 1 dataset, which provides a set of cross-domain language identification trials. Among our experimented systems, the best performance was achieved by using the mel frequency cepstral coefficient ({MFCC}) and pitch features as input and training the {ECAPA}-{TDNN} system with a flow-based regularization technique, which resulted in a Cavg of 0.0631 on the {OLR} 2021 progress set.},
	author = {Kang, Woohyun and Alam, Jahangir and Fathan, Abderrahim},
	langid = {english},
	file = {Kang 等 - Deep learning-based end-to-end spoken language ide.pdf:files/34/Kang 等 - Deep learning-based end-to-end spoken language ide.pdf:application/pdf},
}

@article{noauthor_11-15_nodate,
	title = {11-15 Deep learning based speaker recognition tutorial},
	langid = {english},
	file = {11-15 Deep learning based speaker recognition tuto.pdf:files/35/11-15 Deep learning based speaker recognition tuto.pdf:application/pdf},
}

@inproceedings{zhang_dnn_2016,
	title = {{DNN} speaker adaptation using parameterised sigmoid and {ReLU} hidden activation functions},
	doi = {10.1109/ICASSP.2016.7472689},
	abstract = {This paper investigates the use of parameterised sigmoid and rectified linear unit ({ReLU}) hidden activation functions in deep neural network ({DNN}) speaker adaptation. The sigmoid and {ReLU} parameterisation schemes from a previous study for speaker independent ({SI}) training are used. An adaptive linear factor associated with each sigmoid or {ReLU} hidden unit is used to scale the unit output value and create a speaker dependent ({SD}) model. Hence, {DNN} adaptation becomes re-weighting the importance of different hidden units for every speaker. This adaptation scheme is applied to both hybrid {DNN} acoustic modelling and {DNN}-based bottleneck ({BN}) feature extraction. Experiments using multi-genre British English television broadcast data show that the technique is effective in both directly adapting {DNN} acoustic models and the {BN} features, and combines well with other {DNN} adaptation techniques. Reductions in word error rate are consistently obtained using parameterised sigmoid and {ReLU} activation function for multiple hidden layer adaptation.},
	eventtitle = {2016 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {5300--5304},
	booktitle = {2016 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Zhang, C. and Woodland, P. C.},
	date = {2016-03},
	note = {{ISSN}: 2379-190X},
	keywords = {Acoustics, Adaptation models, Feature extraction, Hidden Markov models, Silicon, Standards, Training},
	file = {IEEE Xplore Abstract Record:files/46/7472689.html:text/html},
}

@misc{sarkar_time-contrastive_2019,
	title = {Time-Contrastive Learning Based {DNN} Bottleneck Features for Text-Dependent Speaker Verification},
	url = {http://arxiv.org/abs/1704.02373},
	doi = {10.48550/arXiv.1704.02373},
	abstract = {In this paper, we present a time-contrastive learning ({TCL}) based bottleneck ({BN})feature extraction method for speech signals with an application to text-dependent ({TD}) speaker verification ({SV}). It is well-known that speech signals exhibit quasi-stationary behavior in and only in a short interval, and the {TCL} method aims to exploit this temporal structure. More specifically, it trains deep neural networks ({DNNs}) to discriminate temporal events obtained by uniformly segmenting speech signals, in contrast to existing {DNN} based {BN} feature extraction methods that train {DNNs} using labeled data to discriminate speakers or pass-phrases or phones or a combination of them. In the context of speaker verification, speech data of fixed pass-phrases are used for {TCL}-{BN} training, while the pass-phrases used for {TCL}-{BN} training are excluded from being used for {SV}, so that the learned features can be considered generic. The method is evaluated on the {RedDots} Challenge 2016 database. Experimental results show that {TCL}-{BN} is superior to the existing speaker and pass-phrase discriminant {BN} features and the Mel-frequency cepstral coefficient feature for text-dependent speaker verification.},
	number = {{arXiv}:1704.02373},
	publisher = {{arXiv}},
	author = {Sarkar, Achintya Kr and Tan, Zheng-Hua},
	urldate = {2023-04-28},
	date = {2019-05-11},
	eprinttype = {arxiv},
	eprint = {1704.02373 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/50/Sarkar 和 Tan - 2019 - Time-Contrastive Learning Based DNN Bottleneck Fea.pdf:application/pdf;arXiv.org Snapshot:files/51/1704.html:text/html},
}

@inproceedings{mclaughlin_study_1999,
	title = {A study of computation speed-{UPS} of the {GMM}-{UBM} speaker recognition system},
	url = {https://www.isca-speech.org/archive/eurospeech_1999/mclaughlin99_eurospeech.html},
	doi = {10.21437/Eurospeech.1999-284},
	abstract = {The Gaussian Mixture Model Universal Background Model ({GMM}-{UBM}) speaker recognition system has demonstrated very high performance in several {NIST} evaluations. Such evaluations, however, are concerned only with classiﬁcation accuracy. In many applications, system effectiveness must be evaluated in light of both accuracy and execution speed. We present here a number of techniques for decreasing computation. Using data from the Switchboard telephone speech corpus, we show that signiﬁcant speed-ups can be obtained while sacriﬁcing surprisingly little accuracy. We expect that these techniques, involving lowering model order as well as processing fewer speech frames, will apply equally well to other recognition systems.},
	eventtitle = {6th European Conference on Speech Communication and Technology (Eurospeech 1999)},
	pages = {1215--1218},
	booktitle = {6th European Conference on Speech Communication and Technology (Eurospeech 1999)},
	publisher = {{ISCA}},
	author = {{McLaughlin}, Jack and Reynolds, Douglas A. and Gleason, Terry},
	urldate = {2023-04-29},
	date = {1999-09-05},
	langid = {english},
	file = {McLaughlin 等 - 1999 - A study of computation speed-UPS of the GMM-UBM sp.pdf:files/54/McLaughlin 等 - 1999 - A study of computation speed-UPS of the GMM-UBM sp.pdf:application/pdf},
}

@inproceedings{fine_hybrid_2001,
	title = {A hybrid {GMM}/{SVM} approach to speaker identification},
	volume = {1},
	doi = {10.1109/ICASSP.2001.940856},
	abstract = {Proposes a classification scheme that incorporates statistical models and support vector machines. A hybrid system which appropriately combines the advantages of both the generative and discriminant model paradigms is described and experimentally evaluated on a text-independent speaker recognition task in matched and mismatched training and test conditions. Our results prove that the combination is beneficial in terms of performance and practical in terms of computation. We report relative improvements of up to 25\% reduction in identification error rate compared to the baseline statistical model.},
	eventtitle = {2001 {IEEE} International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)},
	pages = {417--420 vol.1},
	booktitle = {2001 {IEEE} International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)},
	author = {Fine, S. and Navratil, J. and Gopinath, R.A.},
	date = {2001-05},
	note = {{ISSN}: 1520-6149},
	keywords = {Acoustic testing, Electronic mail, Hidden Markov models, Hybrid power systems, Kernel, Power generation, Robustness, Speaker recognition, Support vector machine classification, Support vector machines},
}

@article{garcia-romero_analysis_nodate,
	title = {Analysis of i-vector Length Normalization in Speaker Recognition Systems},
	abstract = {We present a method to boost the performance of probabilistic generative models that work with i-vector representations. The proposed approach deals with the {nonGaussian} behavior of i-vectors by performing a simple length normalization. This non-linear transformation allows the use of probabilistic models with Gaussian assumptions that yield equivalent performance to that of more complicated systems based on Heavy-Tailed assumptions. Significant performance improvements are demonstrated on the telephone portion of {NIST} {SRE} 2010.},
	author = {Garcia-Romero, Daniel and Espy-Wilson, Carol Y},
	langid = {english},
	file = {Garcia-Romero 和 Espy-Wilson - Analysis of i-vector Length Normalization in Speak.pdf:files/57/Garcia-Romero 和 Espy-Wilson - Analysis of i-vector Length Normalization in Speak.pdf:application/pdf},
}

@inproceedings{bao_incoherent_2013,
	title = {Incoherent training of deep neural networks to de-correlate bottleneck features for speech recognition},
	doi = {10.1109/ICASSP.2013.6639015},
	abstract = {Recently, the hybrid model combining deep neural network ({DNN}) with context-dependent {HMMs} has achieved some dramatic gains over the conventional {GMM}/{HMM} method in many speech recognition tasks. In this paper, we study how to compete with the state-of-the-art {DNN}/{HMM} method under the traditional {GMM}/{HMM} framework. Instead of using {DNN} as acoustic model, we use {DNN} as a front-end bottleneck ({BN}) feature extraction method to decorrelate long feature vectors concatenated from several consecutive speech frames. More importantly, we have proposed two novel incoherent training methods to explicitly de-correlate {BN} features in learning of {DNN}. The first method relies on minimizing coherence of weight matrices in {DNN} while the second one attempts to minimize correlation coefficients of {BN} features calculated in each mini-batch data in {DNN} training. Experimental results on a 70-hr Mandarin transcription task and the 309-hr Switchboard task have shown that the traditional {GMM}/{HMMs} using {BN} features can yield comparable performance as {DNN}/{HMM}. The proposed incoherent training can produce 2-3\% additional gain over the baseline {BN} features. At last, the discriminatively trained {GMM}/{HMMs} using incoherently trained {BN} features have consistently surpassed the state-of-the-art {DNN}/{HMMs} in all evaluated tasks.},
	eventtitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	pages = {6980--6984},
	booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	author = {Bao, Yebo and Jiang, Hui and Dai, Lirong and Liu, Cong},
	date = {2013-05},
	note = {{ISSN}: 2379-190X},
	keywords = {bottleneck features, Correlation, Deep neural networks ({DNN}), Feature extraction, Hidden Markov models, incoherent training, large vocabulary continuous speech recognition ({LVCSR}), Neural networks, nonlinear dimensionality reduction, Speech recognition, Training, Vectors},
}

@inproceedings{peddinti_time_2015,
	title = {A time delay neural network architecture for efficient modeling of long temporal contexts},
	url = {https://www.isca-speech.org/archive/interspeech_2015/peddinti15b_interspeech.html},
	doi = {10.21437/Interspeech.2015-647},
	abstract = {Recurrent neural network architectures have been shown to efﬁciently model long term temporal dependencies between acoustic events. However the training time of recurrent networks is higher than feedforward networks due to the sequential nature of the learning algorithm. In this paper we propose a time delay neural network architecture which models long term temporal dependencies with training times comparable to standard feed-forward {DNNs}. The network uses sub-sampling to reduce computation during training. On the Switchboard task we show a relative improvement of 7.3\% over the baseline {DNN} model. We present results on several {LVCSR} tasks with training data ranging from 3 to 1800 hours to show the effectiveness of the {TDNN} architecture in learning wider temporal dependencies in both small and large data scenarios, with an average relative improvement of 5.5\%.},
	eventtitle = {Interspeech 2015},
	pages = {3214--3218},
	booktitle = {Interspeech 2015},
	publisher = {{ISCA}},
	author = {Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
	urldate = {2023-04-29},
	date = {2015-09-06},
	langid = {english},
	file = {Peddinti 等 - 2015 - A time delay neural network architecture for effic.pdf:files/60/Peddinti 等 - 2015 - A time delay neural network architecture for effic.pdf:application/pdf},
}

@inproceedings{okabe_attentive_2018,
	title = {Attentive Statistics Pooling for Deep Speaker Embedding},
	url = {http://arxiv.org/abs/1803.10963},
	doi = {10.21437/Interspeech.2018-993},
	abstract = {This paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the {NIST} {SRE} 2012 and the {VoxCeleb} data sets shows that it reduces equal error rates ({EERs}) from the conventional method by 7.5\% and 8.1\%, respectively.},
	pages = {2252--2256},
	booktitle = {Interspeech 2018},
	author = {Okabe, Koji and Koshinaka, Takafumi and Shinoda, Koichi},
	urldate = {2023-04-29},
	date = {2018-09-02},
	eprinttype = {arxiv},
	eprint = {1803.10963 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:files/64/Okabe 等 - 2018 - Attentive Statistics Pooling for Deep Speaker Embe.pdf:application/pdf;arXiv.org Snapshot:files/65/1803.html:text/html},
}


